{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm\n",
    "* [PCA with Text](https://github.com/silvernine209/nyc19_ds20/blob/master/curriculum/project-04/svd-pca/PCA_with_text_ex.ipynb)\n",
    "\n",
    "Scraping\n",
    "* [Scrape Reddit Using API](https://towardsdatascience.com/scraping-reddit-data-1c0af3040768)  \n",
    "* [Google Search Operators](https://ahrefs.com/blog/google-advanced-search-operators/)\n",
    "\n",
    "API\n",
    "* [CryptoCompare](https://www.cryptocompare.com)\n",
    "\n",
    "MongoDB & AWS\n",
    "* [Allow Python to connect to MongoDB on AWS](https://github.com/silvernine209/nyc19_ds20/blob/master/curriculum/project-04/mongodb-prep/python_to_aws_mongo_setup.md) \n",
    "* [MongoDB Exercises](https://github.com/silvernine209/nyc19_ds20/tree/master/curriculum/project-04/mongodb-lab)\n",
    "\n",
    "Cryptocurrency\n",
    "* [Bitcoin Transaction Time](https://themoneymongers.com/bitcoin-transaction-time/) ~ 10 min  \n",
    "* [Crypto Sentiment Analysis Guide #1](https://hackernoon.com/sentiment-analysis-in-cryptocurrency-9abb40005d15)\n",
    "* [Bitcoin Graph](https://www.coindesk.com/price/bitcoin)\n",
    "\n",
    "Additional Support\n",
    "* [NLP Resources](https://github.com/stepthom/text_mining_resources)  \n",
    "* [Practitioner's NLP Guide](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72)\n",
    "* [Metis Project 4 Folder](https://github.com/silvernine209/nyc19_ds20/tree/master/curriculum/project-04)  \n",
    "* [Udacity Project Customer Segmentation](https://github.com/silvernine209/Udacity-Projects/blob/master/Segment%20Customers/customer_segments.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as PCA : LSA, SVD, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date,timedelta,datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load webpage's url and load it into soup\n",
    "def load_soup(url):\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"}\n",
    "    response = requests.get(url,headers=headers)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    return soup\n",
    "\n",
    "# Scrape bitcoin news data\n",
    "def scrape_bitcoin_news_google(start_date,end_date,num_per_page):\n",
    "    # Create \"news_data\" folder if it's not there\n",
    "    git_folder_location = os.path.abspath(os.path.dirname('bitcoin_trader'))\n",
    "    if 'news_data' not in os.listdir(git_folder_location):\n",
    "        !mkdir 'news_data'\n",
    "\n",
    "    # already scraped (list from what's already saved in the folder)\n",
    "    done_list = os.listdir(git_folder_location+'/news_data/')\n",
    "    if '.DS_Store' in done_list:\n",
    "        done_list.remove('.DS_Store')\n",
    "    \n",
    "    # Using trange to have a progress bar to gauge scraping time\n",
    "    for i in trange((end_date-start_date).days):    \n",
    "        # Month, day, and year to be embedded into the url\n",
    "        M=start_date.month\n",
    "        D=start_date.day\n",
    "        Y=start_date.year \n",
    "        \n",
    "        # File name to save pickle file and not to scrape if already scraped\n",
    "        filename = 'google_news_{}_{}_{}.pkl'.format(M,D,Y)\n",
    "        \n",
    "        # if file isn't scraped, go ahead and scrape\n",
    "        if filename not in done_list:\n",
    "            # Load data\n",
    "            base_url = 'https://www.google.com/search?q=cryptocurrency+or+bitcoin&num=40&rlz=1C5CHFA_enUS849US849&biw=573&bih=717&source=lnt&tbs=cdr%3A1%2Ccd_min%3A{}%2F{}%2F{}%2Ccd_max%3A{}%2F{}%2F{}&tbm=nws&num={}'\n",
    "            url= base_url.format(M,D,Y,M,D,Y,num_per_page)\n",
    "            soup = load_soup(url)\n",
    "            \n",
    "            # Empty lists for DataFrame\n",
    "            publishers = []\n",
    "            titles = []\n",
    "            intros = []\n",
    "            dates = []  \n",
    "\n",
    "            # Append data to list\n",
    "            publishers += [publisher.text for publisher in soup.find_all('div', attrs = {'class': 'pDavDe RGRr8e'})]\n",
    "            titles += [title.text for title in soup.find_all('div', attrs = {'class': 'phYMDf nDgy9d'})]\n",
    "            intros += [intro.text for intro in soup.find_all('div', attrs = {'class': 'eYN3rb'})]\n",
    "            dates += [start_date]*num_per_page\n",
    "\n",
    "            # Turn data into DataFrame\n",
    "            df = pd.DataFrame({'date':dates,'publisher':publishers,'title':titles,'intro':intros})\n",
    "\n",
    "            # Pickle scraped data\n",
    "            with open('news_data/'+filename, 'wb') as picklefile:\n",
    "                pickle.dump(df, picklefile)\n",
    "\n",
    "            # Move onto next day\n",
    "            start_date+=timedelta(days=1)\n",
    "        # If file has been scraped, continue\n",
    "        else:\n",
    "            # Move onto next day\n",
    "            print(\"{} has already been scraped.\".format(filename))\n",
    "            start_date+=timedelta(days=1)\n",
    "\n",
    "# Load all scraped pickle files and combine it to one dataframe\n",
    "def combine_pickle_files(folder_name): \n",
    "    # Location of git folder\n",
    "    git_folder_location = os.path.abspath(os.path.dirname('bitcoin_trader'))\n",
    "\n",
    "    # list of pickled files\n",
    "    pickle_list = os.listdir(git_folder_location+'/'+folder_name+'/')\n",
    "    if '.DS_Store' in pickle_list:\n",
    "        pickle_list.remove('.DS_Store')\n",
    "\n",
    "    # Create a DataFrame to dump all individual DataFrames from scraped data\n",
    "    with open(folder_name+'/'+pickle_list[0], 'rb') as picklefile: \n",
    "        df = pickle.load(picklefile)    \n",
    "    df_merged = pd.DataFrame(columns=df.keys())\n",
    "\n",
    "    for file in pickle_list:\n",
    "        with open(folder_name+'/'+file, 'rb') as picklefile: \n",
    "            df = pickle.load(picklefile)\n",
    "        df_merged = pd.concat([df_merged,df],ignore_index=True,axis=0)\n",
    "    return df_merged\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 2669.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google_news_8_1_2019.pkl has already been scraped.\n",
      "google_news_8_2_2019.pkl has already been scraped.\n",
      "google_news_8_3_2019.pkl has already been scraped.\n",
      "google_news_8_4_2019.pkl has already been scraped.\n",
      "google_news_8_5_2019.pkl has already been scraped.\n",
      "google_news_8_6_2019.pkl has already been scraped.\n",
      "google_news_8_7_2019.pkl has already been scraped.\n",
      "google_news_8_8_2019.pkl has already been scraped.\n",
      "google_news_8_9_2019.pkl has already been scraped.\n",
      "google_news_8_10_2019.pkl has already been scraped.\n",
      "google_news_8_11_2019.pkl has already been scraped.\n",
      "google_news_8_12_2019.pkl has already been scraped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Scrape Data by passing in start_date, end_date, num_articles_per_day\n",
    "scrape_bitcoin_news_google(date(2019, 8, 1),date(2019, 8, 13),40) #date.today()\n",
    "   \n",
    "# Load News Data\n",
    "df_news_raw = combine_pickle_files('news_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
