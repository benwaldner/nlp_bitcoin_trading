{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm\n",
    "* [PCA with Text](https://github.com/silvernine209/nyc19_ds20/blob/master/curriculum/project-04/svd-pca/PCA_with_text_ex.ipynb)\n",
    "\n",
    "Scraping\n",
    "* [Scrape Reddit Using API](https://towardsdatascience.com/scraping-reddit-data-1c0af3040768)  \n",
    "* [Google Search Operators](https://ahrefs.com/blog/google-advanced-search-operators/)\n",
    "\n",
    "API\n",
    "* [CryptoCompare](https://www.cryptocompare.com)\n",
    "\n",
    "MongoDB & AWS\n",
    "* [Allow Python to connect to MongoDB on AWS](https://github.com/silvernine209/nyc19_ds20/blob/master/curriculum/project-04/mongodb-prep/python_to_aws_mongo_setup.md) \n",
    "* [MongoDB Exercises](https://github.com/silvernine209/nyc19_ds20/tree/master/curriculum/project-04/mongodb-lab)\n",
    "\n",
    "Cryptocurrency\n",
    "* [Bitcoin Transaction Time](https://themoneymongers.com/bitcoin-transaction-time/) ~ 10 min  \n",
    "* [Crypto Sentiment Analysis Guide #1](https://hackernoon.com/sentiment-analysis-in-cryptocurrency-9abb40005d15)\n",
    "* [Bitcoin Graph](https://www.coindesk.com/price/bitcoin)\n",
    "\n",
    "Additional Support\n",
    "* [NLP Resources](https://github.com/stepthom/text_mining_resources)  \n",
    "* [Practitioner's NLP Guide](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72)\n",
    "* [Metis Project 4 Folder](https://github.com/silvernine209/nyc19_ds20/tree/master/curriculum/project-04)  \n",
    "* [Udacity Project Customer Segmentation](https://github.com/silvernine209/Udacity-Projects/blob/master/Segment%20Customers/customer_segments.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as PCA : LSA, SVD, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping\n",
    "from datetime import date,timedelta,datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import requests\n",
    "# from selenium import webdriver\n",
    "# from seleniumrequests import Chrome\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all scraped pickle files and combine it to one dataframe\n",
    "def combine_pickle_files(folder_name): \n",
    "    # Location of git folder\n",
    "    git_folder_location = os.path.abspath(os.path.dirname('bitcoin_trader'))\n",
    "\n",
    "    # list of pickled files\n",
    "    pickle_list = os.listdir(git_folder_location+'/'+folder_name+'/')\n",
    "    if '.DS_Store' in pickle_list:\n",
    "        pickle_list.remove('.DS_Store')\n",
    "\n",
    "    # Create a DataFrame to dump all individual DataFrames from scraped data\n",
    "    with open(folder_name+'/'+pickle_list[0], 'rb') as picklefile: \n",
    "        df = pickle.load(picklefile)    \n",
    "    df_merged = pd.DataFrame(columns=df.keys())\n",
    "\n",
    "    for file in pickle_list:\n",
    "        with open(folder_name+'/'+file, 'rb') as picklefile: \n",
    "            df = pickle.load(picklefile)\n",
    "        df_merged = pd.concat([df_merged,df],ignore_index=True,axis=0)\n",
    "    return df_merged\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load webpage's url and load it into soup\n",
    "def load_soup(url):\n",
    "    user_agent_list = [\n",
    "       #Chrome\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "        #Firefox\n",
    "        'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "        'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
    "        'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "        'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',\n",
    "        'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "        'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n",
    "        'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n",
    "        'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',\n",
    "        'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'\n",
    "    ]\n",
    "    \n",
    "\n",
    "    \n",
    "    headers = {\"User-Agent\":random.choice(user_agent_list)}\n",
    "    response = requests.get(url,headers=headers)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    return soup,response.status_code\n",
    "\n",
    "# Scrape bitcoin news data\n",
    "def scrape_bitcoin_news_google(start_date,end_date,num_per_page):\n",
    "    # Create \"news_data\" folder if it's not there\n",
    "    git_folder_location = os.path.abspath(os.path.dirname('bitcoin_trader'))\n",
    "    if 'news_data' not in os.listdir(git_folder_location):\n",
    "        !mkdir 'news_data'\n",
    "\n",
    "    # already scraped (list from what's already saved in the folder)\n",
    "    done_list = os.listdir(git_folder_location+'/news_data/')\n",
    "    if '.DS_Store' in done_list:\n",
    "        done_list.remove('.DS_Store')\n",
    "    \n",
    "    # Using trange to have a progress bar to gauge scraping time\n",
    "    for i in trange((end_date-start_date).days):    \n",
    "        # Month, day, and year to be embedded into the url\n",
    "        M=start_date.month\n",
    "        D=start_date.day\n",
    "        Y=start_date.year \n",
    "        \n",
    "        # File name to save pickle file and not to scrape if already scraped\n",
    "        filename = 'google_news_{}_{}_{}.pkl'.format(M,D,Y)\n",
    "        \n",
    "        # if file isn't scraped, go ahead and scrape\n",
    "        if filename not in done_list:\n",
    "            # Load data\n",
    "            base_url = 'https://www.google.com/search?q=cryptocurrency+or+bitcoin&num=40&rlz=1C5CHFA_enUS849US849&biw=573&bih=717&source=lnt&tbs=cdr%3A1%2Ccd_min%3A{}%2F{}%2F{}%2Ccd_max%3A{}%2F{}%2F{}&tbm=nws&num={}'\n",
    "            url= base_url.format(M,D,Y,M,D,Y,num_per_page)\n",
    "            soup,response_code = load_soup(url)\n",
    "            if response_code !=200:\n",
    "                print(\"Blacklisted...?\")\n",
    "                break\n",
    "            \n",
    "            # Empty lists for DataFrame\n",
    "            publishers = []\n",
    "            titles = []\n",
    "            intros = []\n",
    "            dates = []  \n",
    "\n",
    "            # Append data to list\n",
    "            publishers += [publisher.text for publisher in soup.find_all('div', attrs = {'class': 'pDavDe RGRr8e'})]\n",
    "            titles += [title.text for title in soup.find_all('div', attrs = {'class': 'phYMDf nDgy9d'})]\n",
    "            intros += [intro.text for intro in soup.find_all('div', attrs = {'class': 'eYN3rb'})]\n",
    "            dates += [start_date]*len(publishers)\n",
    "            \n",
    "            # Turn data into DataFrame\n",
    "            df = pd.DataFrame({'date':dates,'publisher':publishers,'title':titles,'intro':intros})\n",
    "\n",
    "            # Pickle scraped data\n",
    "            with open('news_data/'+filename, 'wb') as picklefile:\n",
    "                pickle.dump(df, picklefile)\n",
    "\n",
    "            # Move onto next day\n",
    "            start_date+=timedelta(days=1)\n",
    "            \n",
    "            # Randomly sleep any time between 10 and 12 seconds\n",
    "            time.sleep(random.randint(10,12))\n",
    "        # If file has been scraped, continue\n",
    "        else:\n",
    "            # Move onto next day\n",
    "            print(\"{} has already been scraped.\".format(filename))\n",
    "            start_date+=timedelta(days=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Selenium Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "git_folder_location = os.path.abspath(os.path.dirname('bitcoin_trader'))\n",
    "full_path_to_chromedriver = os.path.join(git_folder_location, \"chromedriver\")\n",
    "driver = Chrome(executable_path = full_path_to_chromedriver)\n",
    "\n",
    "# Scrape bitcoin news data\n",
    "def scrape_bitcoin_news_google(start_date,end_date,num_per_page):\n",
    "    # Create \"news_data\" folder if it's not there\n",
    "    git_folder_location = os.path.abspath(os.path.dirname('bitcoin_trader'))\n",
    "    if 'news_data' not in os.listdir(git_folder_location):\n",
    "        !mkdir 'news_data'\n",
    "\n",
    "    # already scraped (list from what's already saved in the folder)\n",
    "    done_list = os.listdir(git_folder_location+'/news_data/')\n",
    "    if '.DS_Store' in done_list:\n",
    "        done_list.remove('.DS_Store')\n",
    "    \n",
    "    # Using trange to have a progress bar to gauge scraping time\n",
    "    for i in trange((end_date-start_date).days):    \n",
    "        # Month, day, and year to be embedded into the url\n",
    "        M=start_date.month\n",
    "        D=start_date.day\n",
    "        Y=start_date.year \n",
    "        \n",
    "        # File name to save pickle file and not to scrape if already scraped\n",
    "        filename = 'google_news_{}_{}_{}.pkl'.format(M,D,Y)\n",
    "        \n",
    "        # if file isn't scraped, go ahead and scrape\n",
    "        if filename not in done_list:\n",
    "            # Load data\n",
    "            base_url = 'https://www.google.com/search?q=cryptocurrency+or+bitcoin&num=40&rlz=1C5CHFA_enUS849US849&biw=573&bih=717&source=lnt&tbs=cdr%3A1%2Ccd_min%3A{}%2F{}%2F{}%2Ccd_max%3A{}%2F{}%2F{}&tbm=nws&num={}'\n",
    "            url= base_url.format(M,D,Y,M,D,Y,num_per_page)\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Empty lists for DataFrame\n",
    "            publishers = []\n",
    "            titles = []\n",
    "            intros = []\n",
    "            dates = []  \n",
    "\n",
    "            # Append data to list\n",
    "            publishers += [publisher.text for publisher in soup.find_all('div', attrs = {'class': 'pDavDe RGRr8e'})]\n",
    "            titles += [title.text for title in soup.find_all('div', attrs = {'class': 'phYMDf nDgy9d'})]\n",
    "            intros += [intro.text for intro in soup.find_all('div', attrs = {'class': 'eYN3rb'})]\n",
    "            dates += [start_date]*len(publishers)\n",
    "            \n",
    "            # Turn data into DataFrame\n",
    "            df = pd.DataFrame({'date':dates,'publisher':publishers,'title':titles,'intro':intros})\n",
    "\n",
    "            # Pickle scraped data\n",
    "            with open('news_data/'+filename, 'wb') as picklefile:\n",
    "                pickle.dump(df, picklefile)\n",
    "\n",
    "            # Move onto next day\n",
    "            start_date+=timedelta(days=1)\n",
    "        # If file has been scraped, continue\n",
    "        else:\n",
    "            # Move onto next day\n",
    "            print(\"{} has already been scraped.\".format(filename))\n",
    "            start_date+=timedelta(days=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.google.com/search?q=cryptocurrency+or+bitcoin&num=40&rlz=1C5CHFA_enUS849US849&biw=573&bih=717&source=lnt&tbs=cdr%3A1%2Ccd_min%3A8%2F10%2F2019%2Ccd_max%3A8%2F10%2F2019&tbm=nws&num=40'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request #1\n",
      "Skipping. Connnection error\n",
      "Request #2\n",
      "Skipping. Connnection error\n",
      "Request #3\n",
      "Skipping. Connnection error\n",
      "Request #4\n",
      "Skipping. Connnection error\n",
      "Request #5\n"
     ]
    }
   ],
   "source": [
    "from lxml.html import fromstring\n",
    "import requests\n",
    "from itertools import cycle\n",
    "import traceback\n",
    "\n",
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies\n",
    "\n",
    "\n",
    "#If you are copy pasting proxy ips, put in the list below\n",
    "#proxies = ['121.129.127.209:80', '124.41.215.238:45169', '185.93.3.123:8080', '194.182.64.67:3128', '106.0.38.174:8080', '163.172.175.210:3128', '13.92.196.150:8080']\n",
    "proxies = get_proxies()\n",
    "proxy_pool = cycle(proxies)\n",
    "\n",
    "\n",
    "for i in range(1,11):\n",
    "    #Get a proxy from the pool\n",
    "    proxy = next(proxy_pool)\n",
    "    print(\"Request #%d\"%i)\n",
    "    try:\n",
    "        response = requests.get(url,proxies={\"http\": proxy, \"https\": proxy})\n",
    "        print(response.json())\n",
    "    except:\n",
    "        #Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work. \n",
    "        #We will just skip retries as its beyond the scope of this tutorial and we are only downloading a single url \n",
    "        print(\"Skipping. Connnection error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.google.com/search?q=cryptocurrency+or+bitcoin&num=40&rlz=1C5CHFA_enUS849US849&biw=573&bih=717&source=lnt&tbs=cdr%3A1%2Ccd_min%3A8%2F10%2F2019%2Ccd_max%3A8%2F10%2F2019&tbm=nws&num=40'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1046/1046 [11:50<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Scrape Data by passing in start_date, end_date, num_articles_per_day\n",
    "scrape_bitcoin_news_google(date(2016, 10, 1),date(2019, 8, 13),40) #date.today()\n",
    "   \n",
    "# Load News Data\n",
    "df_news_raw = combine_pickle_files('news_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
