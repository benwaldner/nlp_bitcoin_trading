{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Algorithm\n",
    "* [PCA with Text](https://github.com/silvernine209/nyc19_ds20/blob/master/curriculum/project-04/svd-pca/PCA_with_text_ex.ipynb)\n",
    "\n",
    "Scraping\n",
    "* [Scrape Reddit Using API](https://towardsdatascience.com/scraping-reddit-data-1c0af3040768)  \n",
    "* [Google Search Operators](https://ahrefs.com/blog/google-advanced-search-operators/)\n",
    "\n",
    "API\n",
    "* [CryptoCompare](https://www.cryptocompare.com)\n",
    "\n",
    "MongoDB & AWS\n",
    "* [Allow Python to connect to MongoDB on AWS](https://github.com/silvernine209/nyc19_ds20/blob/master/curriculum/project-04/mongodb-prep/python_to_aws_mongo_setup.md) \n",
    "* [MongoDB Exercises](https://github.com/silvernine209/nyc19_ds20/tree/master/curriculum/project-04/mongodb-lab)\n",
    "\n",
    "Cryptocurrency\n",
    "* [Bitcoin Transaction Time](https://themoneymongers.com/bitcoin-transaction-time/) ~ 10 min  \n",
    "* [Crypto Sentiment Analysis Guide #1](https://hackernoon.com/sentiment-analysis-in-cryptocurrency-9abb40005d15)\n",
    "* [Bitcoin Graph](https://www.coindesk.com/price/bitcoin)\n",
    "\n",
    "Additional Support\n",
    "* [NLP Resources](https://github.com/stepthom/text_mining_resources)  \n",
    "* [Practitioner's NLP Guide](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72)\n",
    "* [Metis Project 4 Folder](https://github.com/silvernine209/nyc19_ds20/tree/master/curriculum/project-04)  \n",
    "* [Udacity Project Customer Segmentation](https://github.com/silvernine209/Udacity-Projects/blob/master/Segment%20Customers/customer_segments.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Same as PCA : LSA, SVD, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Web Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "\n",
    "# NLP\n",
    "from contractions import CONTRACTION_MAP\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "# !spacy download en_core_web_md\n",
    "# !python -m spacy link /Users/matthewlee/Desktop/Metis/bitcoin_trader/en_core_web_md-2.0.0/en_core_web_md en_core\n",
    "#nlp = spacy.load('en_core_web_md', parse=True, tag=True, entity=True)\n",
    "nlp = en_core_web_md.load(parse=True, tag=True, entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "# Tools\n",
    "import string\n",
    "from datetime import date,timedelta,datetime\n",
    "import unicodedata\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "# Load all scraped pickle files and combine it to one dataframe\n",
    "def combine_pickle_files(folder_name): \n",
    "    # Location of git folder\n",
    "    git_folder_location = os.path.abspath(os.path.dirname('bitcoin_trader'))\n",
    "\n",
    "    # list of pickled files\n",
    "    pickle_list = os.listdir(git_folder_location+'/'+folder_name+'/')\n",
    "    if '.DS_Store' in pickle_list:\n",
    "        pickle_list.remove('.DS_Store')\n",
    "\n",
    "    # Create a DataFrame to dump all individual DataFrames from scraped data\n",
    "    with open(folder_name+'/'+pickle_list[0], 'rb') as picklefile: \n",
    "        df = pickle.load(picklefile)    \n",
    "    df_merged = pd.DataFrame(columns=df.keys())\n",
    "\n",
    "    for file in pickle_list:\n",
    "        with open(folder_name+'/'+file, 'rb') as picklefile: \n",
    "            df = pickle.load(picklefile)\n",
    "        df_merged = pd.concat([df_merged,df],ignore_index=True,axis=0)\n",
    "    return df_merged\n",
    "\n",
    "# HTML tags\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "# example) do not -> don't. I would -> I'd\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "# standardized into ASCII characters. example) converting Ã© to e\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text \n",
    "\n",
    "# Special characters and symbols\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# JUMPS, JUMPED, and JUMPING -> JUMP\n",
    "# Multiple Stemmers : PorterStemmer, LancasterStemmer, SnowballStemmer WordNetLemmatizer\n",
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "# # Remove punctuation\n",
    "# re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)\n",
    "\n",
    "# # Lower Case\n",
    "# clean_text = clean_text.lower()\n",
    "\n",
    "# # Removes all words containing digits\n",
    "# clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\n",
    "\n",
    "# # Stop words\n",
    "# from nltk.corpus import stopwords\n",
    "# set(stopwords.words('english'))\n",
    "\n",
    "# # Speech Tagging\n",
    "# from nltk.tag import pos_tag\n",
    "# my_text = \"James Smith lives in the United States.\"\n",
    "# tokens = pos_tag(word_tokenize(my_text))\n",
    "# print(tokens)\n",
    "# nltk.help.upenn_tagset()\n",
    "\n",
    "# # Named Entity\n",
    "# from nltk.chunk import ne_chunk\n",
    "# my_text = \"James Smith lives in the United States.\"\n",
    "# tokens = pos_tag(word_tokenize(my_text)) # this labels each word as a part of speech\n",
    "# entities = ne_chunk(tokens) # this extracts entities from the list of words\n",
    "# entities.draw()\n",
    "\n",
    "# # Compoun Term Extraction\n",
    "# from nltk.tokenize import MWETokenizer # multi-word expression\n",
    "# my_text = \"You all are the greatest students of all time.\"\n",
    "# mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
    "# mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
    "# mwe_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = doc\n",
    "            #doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### BeautifulSoup Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load webpage's url and load it into soup\n",
    "def load_soup(url):\n",
    "    user_agent_list = [\n",
    "       #Chrome\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "         ]\n",
    "    \n",
    "\n",
    "    \n",
    "    headers = {\"User-Agent\":random.choice(user_agent_list)}\n",
    "    response = requests.get(url,headers=headers)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    return soup,response.status_code\n",
    "\n",
    "# Scrape bitcoin news data\n",
    "def scrape_bitcoin_news_google(start_date,end_date,num_per_page):\n",
    "    # Create \"news_data\" folder if it's not there\n",
    "    git_folder_location = os.path.abspath(os.path.dirname('bitcoin_trader'))\n",
    "    if 'news_data' not in os.listdir(git_folder_location):\n",
    "        !mkdir 'news_data'\n",
    "\n",
    "    # already scraped (list from what's already saved in the folder)\n",
    "    done_list = os.listdir(git_folder_location+'/news_data/')\n",
    "    if '.DS_Store' in done_list:\n",
    "        done_list.remove('.DS_Store')\n",
    "    \n",
    "    # Using trange to have a progress bar to gauge scraping time\n",
    "    for i in trange((end_date-start_date).days):    \n",
    "        # Month, day, and year to be embedded into the url\n",
    "        M=start_date.month\n",
    "        D=start_date.day\n",
    "        Y=start_date.year \n",
    "        \n",
    "        # File name to save pickle file and not to scrape if already scraped\n",
    "        filename = 'google_news_{}_{}_{}.pkl'.format(M,D,Y)\n",
    "        \n",
    "        # if file isn't scraped, go ahead and scrape\n",
    "        if filename not in done_list:\n",
    "            # Load data\n",
    "            base_url = 'https://www.google.com/search?q=cryptocurrency+or+bitcoin&num=40&rlz=1C5CHFA_enUS849US849&biw=573&bih=717&source=lnt&tbs=cdr%3A1%2Ccd_min%3A{}%2F{}%2F{}%2Ccd_max%3A{}%2F{}%2F{}&tbm=nws&num={}'\n",
    "            url= base_url.format(M,D,Y,M,D,Y,num_per_page)\n",
    "            soup,response_code = load_soup(url)\n",
    "            if response_code !=200:\n",
    "                print(\"Blacklisted...?\")\n",
    "                print(start_date)\n",
    "                break\n",
    "            \n",
    "            # Empty lists for DataFrame\n",
    "            publishers = []\n",
    "            titles = []\n",
    "            intros = []\n",
    "            dates = []  \n",
    "\n",
    "            # Append data to list\n",
    "            publishers += [publisher.text for publisher in soup.find_all('div', attrs = {'class': 'pDavDe RGRr8e'})]\n",
    "            titles += [title.text for title in soup.find_all('div', attrs = {'class': 'phYMDf nDgy9d'})]\n",
    "            intros += [intro.text for intro in soup.find_all('div', attrs = {'class': 'eYN3rb'})]\n",
    "            dates += [start_date]*len(publishers)\n",
    "            \n",
    "            # Turn data into DataFrame\n",
    "            df = pd.DataFrame({'date':dates,'publisher':publishers,'title':titles,'intro':intros})\n",
    "            if len(df)<1:\n",
    "                print(\"Empty df\")\n",
    "                break\n",
    "            # Pickle scraped data\n",
    "            with open('news_data/'+filename, 'wb') as picklefile:\n",
    "                pickle.dump(df, picklefile)\n",
    "            # Move onto next day\n",
    "            start_date+=timedelta(days=1)\n",
    "            \n",
    "#             # Randomly sleep any time between 10 and 12 seconds\n",
    "#             time.sleep(random.randint(10,12))\n",
    "        # If file has been scraped, continue\n",
    "        else:\n",
    "            # Move onto next day\n",
    "            print(\"{} has already been scraped.\".format(filename))\n",
    "            start_date+=timedelta(days=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scrape Data by passing in start_date, end_date, num_articles_per_day\n",
    "# # Pages before Google blacklists an IP : 73,65,116,97,71,94,80,79,85,88,84,66,79,76,96,92.\n",
    "# scrape_bitcoin_news_google(date(2019, 6, 8),date(2019, 8, 13),40) #date.today()\n",
    "\n",
    "# # Combine individual day pickle files & pickle it\n",
    "# df_news_raw = combine_pickle_files('news_data')\n",
    "# df_news_raw.sort_values(by=['date'],inplace=True)\n",
    "# df_news_raw.reset_index(drop=True,inplace=True)\n",
    "# with open('df_news_raw.pkl','wb') as picklefile:\n",
    "#     pickle.dump(df_news_raw,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df_news_raw\n",
    "with open('df_news_raw.pkl', 'rb') as picklefile: \n",
    "    df_news_raw = pickle.load(picklefile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title & intro\n",
    "df_news_raw['title_intro'] = df_news_raw['title']+'. '+df_news_raw['intro']\n",
    "\n",
    "# Remove '\\n' from corpus\n",
    "df_news_raw['title_intro'] = df_news_raw['title_intro'].apply(lambda x : x.replace('\\n',''))\n",
    "\n",
    "# Cleaned 'title_intro'\n",
    "df_news_raw['title_intro_clean']=normalize_corpus(df_news_raw['title_intro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer & NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc Shape :  (24823, 34871)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     3400\n",
       "22    2609\n",
       "6     2444\n",
       "2     1887\n",
       "10    1662\n",
       "16    1304\n",
       "11    1243\n",
       "24    1127\n",
       "5      995\n",
       "20     924\n",
       "9      817\n",
       "19     779\n",
       "23     625\n",
       "15     577\n",
       "13     561\n",
       "8      556\n",
       "14     480\n",
       "18     451\n",
       "21     414\n",
       "7      398\n",
       "12     380\n",
       "17     328\n",
       "3      321\n",
       "1      316\n",
       "4      225\n",
       "dtype: int64"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Word Model\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create vectorized matrix with stopword\n",
    "vectorizer = CountVectorizer(stop_words=stopword_list)\n",
    "doc_word = vectorizer.fit_transform(df_news_raw['title_intro_clean'])\n",
    "print(\"Doc Shape : \",doc_word.shape)\n",
    "#pd.DataFrame(doc_word.toarray(), index=df_news_raw['title_intro_clean'], columns=vectorizer.get_feature_names()).head()\n",
    "\n",
    "# Define NMF model\n",
    "nmf_model = NMF(n_components=25, init='random', random_state=0)\n",
    "doc_topics = nmf_model.fit_transform(doc_word)\n",
    "\n",
    "# Check cluster distribution\n",
    "doc_cluster = doc_topics.argmax(axis = 1)\n",
    "pd.Series(doc_cluster).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['positions',\n",
       "  'telaviv',\n",
       "  'poets',\n",
       "  'libertarians',\n",
       "  'shy',\n",
       "  'socalled',\n",
       "  'procent'],\n",
       " ['weatherproof',\n",
       "  'drogba',\n",
       "  'rizzo',\n",
       "  'segwitx',\n",
       "  'eretailer',\n",
       "  'cased',\n",
       "  'advising'],\n",
       " ['cashed',\n",
       "  'recruiting',\n",
       "  'individually',\n",
       "  'studios',\n",
       "  'cryptoonly',\n",
       "  'riflettono',\n",
       "  'inexpensive'],\n",
       " ['substantially', 'pada', 'glad', 'insolvency', 'moby', 'boyart', 'balks'],\n",
       " ['sights', 'enjoys', 'rizzo', 'toda', 'positions', 'stirs', 'master'],\n",
       " ['hollywood',\n",
       "  'segwitx',\n",
       "  'bursa',\n",
       "  'autotrigger',\n",
       "  'episerver',\n",
       "  'backward',\n",
       "  'understand'],\n",
       " ['celect',\n",
       "  'episerver',\n",
       "  'flies',\n",
       "  'seleccionar',\n",
       "  'trapheavy',\n",
       "  'sink',\n",
       "  'segwitx'],\n",
       " ['perishable',\n",
       "  'flies',\n",
       "  'cryptoonly',\n",
       "  'sanctionshit',\n",
       "  'posted',\n",
       "  'disapproval',\n",
       "  'autotrigger'],\n",
       " ['punishments',\n",
       "  'spooked',\n",
       "  'pascal',\n",
       "  'telaviv',\n",
       "  'beleggingsonderneming',\n",
       "  'cpy',\n",
       "  'bigshot'],\n",
       " ['conduit',\n",
       "  'corruption',\n",
       "  'spooked',\n",
       "  'cryptoonly',\n",
       "  'merged',\n",
       "  'disposal',\n",
       "  'inbrengen'],\n",
       " ['wading',\n",
       "  'redundant',\n",
       "  'switched',\n",
       "  'xmrig',\n",
       "  'disposal',\n",
       "  'second',\n",
       "  'fabricate'],\n",
       " ['cloudhashing',\n",
       "  'khromaev',\n",
       "  'segwitx',\n",
       "  'bursa',\n",
       "  'forgiving',\n",
       "  'tracked',\n",
       "  'riskyet'],\n",
       " ['naeem', 'switched', 'master', 'cpy', 'beleggingsonderneming', 'mem', 'cip'],\n",
       " ['outcome', 'telaviv', 'cpy', 'bappebti', 'tracked', 'shitting', 'hsbcs'],\n",
       " ['australian',\n",
       "  'khromaev',\n",
       "  'forgiving',\n",
       "  'tracked',\n",
       "  'segwitx',\n",
       "  'juncker',\n",
       "  'disapproval'],\n",
       " ['chelseas', 'nha', 'unified', 'fabricate', 'rallied', 'teacher', 'lil'],\n",
       " ['surrounding', 'goxrising', 'xmrig', 'och', 'fabricate', 'claus', 'memory'],\n",
       " ['expand',\n",
       "  'autotrigger',\n",
       "  'cased',\n",
       "  'shitting',\n",
       "  'paltrow',\n",
       "  'termination',\n",
       "  'eretailer'],\n",
       " ['privatizing',\n",
       "  'shitting',\n",
       "  'sanctionshit',\n",
       "  'cpy',\n",
       "  'fabricate',\n",
       "  'thinkers',\n",
       "  'cryptoonly'],\n",
       " ['sne',\n",
       "  'yumc',\n",
       "  'chevron',\n",
       "  'understand',\n",
       "  'innovation',\n",
       "  'vavilov',\n",
       "  'strongins'],\n",
       " ['proportions', 'spring', 'bodies', 'segwitx', 'robb', 'bursa', 'creating'],\n",
       " ['swap', 'jpmorgans', 'incomes', 'genomics', 'wayfairs', 'amaze', 'make'],\n",
       " ['catastrophic',\n",
       "  'flies',\n",
       "  'paltrow',\n",
       "  'bristolmyers',\n",
       "  'understand',\n",
       "  'makan',\n",
       "  'commands'],\n",
       " ['fortinet',\n",
       "  'stalled',\n",
       "  'glyphs',\n",
       "  'destruction',\n",
       "  'findom',\n",
       "  'moore',\n",
       "  'fabricate'],\n",
       " ['graphs', 'xmrig', 'inbox', 'carnegiebrown', 'catz', 'flies', 'switched']]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = model.components_.argsort(axis=1)[:,-1:-8:-1]\n",
    "topic_words = [[list(vectorizer.vocabulary_.keys())[e-1] for e in l] for l in t]\n",
    "#topic_words = [[vectorizer.get_feature_names()[e-1] for e in l] for l in t]\n",
    "topic_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-idf & NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2541\n",
       "22    1646\n",
       "16    1388\n",
       "5     1323\n",
       "11    1309\n",
       "2     1188\n",
       "20    1176\n",
       "23    1021\n",
       "12    1007\n",
       "10     974\n",
       "18     967\n",
       "4      957\n",
       "15     949\n",
       "9      932\n",
       "6      898\n",
       "19     834\n",
       "13     818\n",
       "3      781\n",
       "1      724\n",
       "21     652\n",
       "8      648\n",
       "14     603\n",
       "17     603\n",
       "7      538\n",
       "24     346\n",
       "dtype: int64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# new TF-IDF Vectorizer\n",
    "cv_tfidf = TfidfVectorizer(stop_words=stopword_list)\n",
    "X_tfidf = cv_tfidf.fit_transform(df_news_raw['title_intro_clean'])\n",
    "#pd.DataFrame(X_tfidf, columns=cv_tfidf.get_feature_names())\n",
    "\n",
    "# Define NMF model\n",
    "nmf_model = NMF(n_components=25, init='random', random_state=0)\n",
    "doc_topics = nmf_model.fit_transform(X_tfidf)\n",
    "\n",
    "# Check cluster distribution\n",
    "doc_cluster = doc_topics.argmax(axis = 1)\n",
    "pd.Series(doc_cluster).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['positions',\n",
       "  'telaviv',\n",
       "  'poets',\n",
       "  'libertarians',\n",
       "  'shy',\n",
       "  'socalled',\n",
       "  'procent'],\n",
       " ['weatherproof',\n",
       "  'drogba',\n",
       "  'rizzo',\n",
       "  'segwitx',\n",
       "  'eretailer',\n",
       "  'cased',\n",
       "  'advising'],\n",
       " ['cashed',\n",
       "  'recruiting',\n",
       "  'individually',\n",
       "  'studios',\n",
       "  'cryptoonly',\n",
       "  'riflettono',\n",
       "  'inexpensive'],\n",
       " ['substantially', 'pada', 'glad', 'insolvency', 'moby', 'boyart', 'balks'],\n",
       " ['sights', 'enjoys', 'rizzo', 'toda', 'positions', 'stirs', 'master'],\n",
       " ['hollywood',\n",
       "  'segwitx',\n",
       "  'bursa',\n",
       "  'autotrigger',\n",
       "  'episerver',\n",
       "  'backward',\n",
       "  'understand'],\n",
       " ['celect',\n",
       "  'episerver',\n",
       "  'flies',\n",
       "  'seleccionar',\n",
       "  'trapheavy',\n",
       "  'sink',\n",
       "  'segwitx'],\n",
       " ['perishable',\n",
       "  'flies',\n",
       "  'cryptoonly',\n",
       "  'sanctionshit',\n",
       "  'posted',\n",
       "  'disapproval',\n",
       "  'autotrigger'],\n",
       " ['punishments',\n",
       "  'spooked',\n",
       "  'pascal',\n",
       "  'telaviv',\n",
       "  'beleggingsonderneming',\n",
       "  'cpy',\n",
       "  'bigshot'],\n",
       " ['conduit',\n",
       "  'corruption',\n",
       "  'spooked',\n",
       "  'cryptoonly',\n",
       "  'merged',\n",
       "  'disposal',\n",
       "  'inbrengen'],\n",
       " ['wading',\n",
       "  'redundant',\n",
       "  'switched',\n",
       "  'xmrig',\n",
       "  'disposal',\n",
       "  'second',\n",
       "  'fabricate'],\n",
       " ['cloudhashing',\n",
       "  'khromaev',\n",
       "  'segwitx',\n",
       "  'bursa',\n",
       "  'forgiving',\n",
       "  'tracked',\n",
       "  'riskyet'],\n",
       " ['naeem', 'switched', 'master', 'cpy', 'beleggingsonderneming', 'mem', 'cip'],\n",
       " ['outcome', 'telaviv', 'cpy', 'bappebti', 'tracked', 'shitting', 'hsbcs'],\n",
       " ['australian',\n",
       "  'khromaev',\n",
       "  'forgiving',\n",
       "  'tracked',\n",
       "  'segwitx',\n",
       "  'juncker',\n",
       "  'disapproval'],\n",
       " ['chelseas', 'nha', 'unified', 'fabricate', 'rallied', 'teacher', 'lil'],\n",
       " ['surrounding', 'goxrising', 'xmrig', 'och', 'fabricate', 'claus', 'memory'],\n",
       " ['expand',\n",
       "  'autotrigger',\n",
       "  'cased',\n",
       "  'shitting',\n",
       "  'paltrow',\n",
       "  'termination',\n",
       "  'eretailer'],\n",
       " ['privatizing',\n",
       "  'shitting',\n",
       "  'sanctionshit',\n",
       "  'cpy',\n",
       "  'fabricate',\n",
       "  'thinkers',\n",
       "  'cryptoonly'],\n",
       " ['sne',\n",
       "  'yumc',\n",
       "  'chevron',\n",
       "  'understand',\n",
       "  'innovation',\n",
       "  'vavilov',\n",
       "  'strongins'],\n",
       " ['proportions', 'spring', 'bodies', 'segwitx', 'robb', 'bursa', 'creating'],\n",
       " ['swap', 'jpmorgans', 'incomes', 'genomics', 'wayfairs', 'amaze', 'make'],\n",
       " ['catastrophic',\n",
       "  'flies',\n",
       "  'paltrow',\n",
       "  'bristolmyers',\n",
       "  'understand',\n",
       "  'makan',\n",
       "  'commands'],\n",
       " ['fortinet',\n",
       "  'stalled',\n",
       "  'glyphs',\n",
       "  'destruction',\n",
       "  'findom',\n",
       "  'moore',\n",
       "  'fabricate'],\n",
       " ['graphs', 'xmrig', 'inbox', 'carnegiebrown', 'catz', 'flies', 'switched']]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = model.components_.argsort(axis=1)[:,-1:-8:-1]\n",
    "topic_words = [[list(cv_tfidf.vocabulary_.keys())[e-1] for e in l] for l in t]\n",
    "#topic_words = [[vectorizer.get_feature_names()[e-1] for e in l] for l in t]\n",
    "topic_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
